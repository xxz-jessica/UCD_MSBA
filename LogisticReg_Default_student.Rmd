title: **"LogisticReg_Default"**
output: html_notebook


Load Libraries

```{r}
rm(list=ls())
library(readxl)
library(Hmisc)
library(MASS)
library(caret)
library(regclass)
library(ISLR)
library(boot)
library(vcd)
library(pROC)
```


After loading the libraries, load the excel file containing data on Defaults

```{r}
mydata<-read_excel("Default_logistic.xlsx")

#mydata<-read_excel("/Users/Ashwin/Box Sync/MSBA/Director/Intro to Business Analytics/2019/Classes/Class 7_2019/Default_logistic.xlsx") ##Macbook
#mydata<-read_excel("C:/Users/Ashwin/Box Sync/MSBA/Director/Intro to Business Analytics/2019/Classes/Class 7_2019/Default_logistic.xlsx") ##Home Desktop


summary(mydata) #summary of the data
```
You could also use

```{r}
str(mydata)
```


What do you notice? Looks like the Variables default and student are *character* and not *numeric*. Let's look at the values they take.

```{r}
head(mydata)
```

head alone might not display _all possible values_. Let's find all the possible values

```{r}
levels(as.factor(mydata$default))
levels(as.factor(mydata$student))

```


Looks like both have only two levels and are Yes/No variables. Let's convert all No to 0 and Yes to 1. To do so, we create two new variables in the same mydata table. The two variables are labelled default2 and student2.


```{r}
mydata$default2<- ifelse(mydata$default == "No",0,1)
mydata$student2<- ifelse(mydata$student == "No",0,1)
```

Confirm the change by looking at the data again. And get the summary.

```{r}
head(mydata)

#levels(as.factor(mydata$default2))
#levels(as.factor(mydata$student2)) #if you wanted to recheck the levels
```
```{r}
summary(mydata)
```

Looks like it worked!

Now let's code the model. We want to predict default. The independent variables in our dataset are 'student=Yes/No', 'balance' and 'income'. The default variable is discrete and has only two levels: {0,1}. Unlike in a regression, where the dependent variable is continuous, default being discrete does not lend itself to standard regression analysis.

```{r}
#Logistic Regression Model Estimation
mylogit<-glm(default2~student2+balance+income,data=mydata,family=binomial(link="logit"))
```


Let's output the results

```{r}
#coefficients
summary(mylogit)
```

While the p-values indicate confidence, we could also output the 2.5% and 97.5% confindence intervals
```{r}
#Confidence Intervals
Confidence=confint(mylogit) 
Confidence
```

Another way to interpret the parameter estimates would be to compute the Odds Ratios

```{r}
#Odds Ratio Calculation, including confidence intervals
oddsr=round(exp(cbind(OddsRatio=coef(mylogit),confint(mylogit))),4)

oddsr


```
How would you interpret the Odd Ratio?

_Write your answer below_






What's a good measure of fit?

```{r}
confmat<-confusion_matrix(mylogit) #Predict True/False Positive/Negative (TP,TN,FP.FN)
confmat
```



Let's measure Cohen's Kappa -- degree of agreement between observed and predicted

	          Obs
	       _________
	        Yes| No
	        ______
Pred|	Yes| a | b
    |  No| c | d

 	          Obs
	       _________
	        +  | -
	        ______
Pred|	+  | a | b
    | -  | c | d


a=True Positive=(1,1)
d=True Negative=(0,0)
b=False Positive=(0,1)
c=False Negative=(1,0)

p0=(a+d)/(a+b+c+d) #Accuracy
pY=(a+b)*(a+c)/(a+b+c+d)^2
pN=(c+d)*(b+d)/(a+b+c+d)^2
pe=pY+pN #hypothetical probability of chance agreement
kappa=(p0-pe)/(1-pe)


We can also include

 	          Obs
	       _________
	         +  | -
	        ________
Pred|	+  | TP | FP
    | -  | FN | TN


Precision = TP/(TP+FP) == True positives by total true predicted; how many of the positively classified were relevant. 

Recall = Sensitivity = TP/(TP+FN) == True Positives by total true in reality; how good a test is at detecting the positives.

Specificity = TN/(TN+FP) == True negatives by total negatives in reality; how good a test is at avoiding false alarms. 

F 1 Score = (2*recall*precision)/(recall+precision)


```{r}
#first get predicted values
preddata<-with(mydata,data.frame(ID,default,student,balance,income,default2,student2))
probdefault<-predict(mylogit,newdata=preddata,type="response")
preddefault<-ifelse(probdefault > 0.5, 1,0) #at what level should we say prob(default)=1

#Let's determine Accuracy manually first
missclass<-preddefault!=mydata$default2
misclasserror<-round(mean(preddefault!=mydata$default2),4)
print(paste('Accuracy',1-misclasserror)) #To determine accuracy manually


```

You could also use the command Confusion Matrix
```{r}
confMat2<-confusionMatrix(data = as.factor(preddefault),reference = as.factor(mydata$default2),positive = "1")
confMat2 ###Note, because of how this matrix is strutured, 0,0 becomes true positive -- thus we specify positive as 1
```




Next, let's predict the probabilities of default at different cutoffs (instead of 0.5). How does accuracy change?

```{r}
preddata<-with(mydata,data.frame(ID,default,student,balance,income,default2,student2))
probdefault<-predict(mylogit,newdata=preddata,type="response")
preddefault<-ifelse(probdefault > 0.7, 1,0) #at what level should we say prob(default)=1
confMat2<-confusionMatrix(data = as.factor(preddefault),reference = as.factor(mydata$default2),positive = "1") #try from 0.2 #to 0.7
confMat2 ###Note, because of how this matrix is strutured, 0,0 becomes true positive -- thus we specify positive as 1
(2*confMat2[['byClass']][["Pos Pred Value"]]*confMat2[['byClass']][["Sensitivity"]])/(confMat2[['byClass']][["Pos Pred Value"]]+confMat2[['byClass']][["Sensitivity"]])

```

What do you notice?


_<<<<Write your answer below>>>>_




**Extensions** 

**Resampling Methods**

Leave One Out Cross Validation -- LOOCV


```{r}

set.seed(20)
sample_siz = floor(0.75*nrow(mydata))  # creates a value for dividing the data into train and test. In this case the value is defined as 75% of the number of rows in the dataset
sample_siz #how big?
train_index = sample(seq_len(nrow(mydata)),size = sample_siz)# Randomly identifies therows equal to sample size ( defined in previous instruction) from  all the rows of Smarket dataset and stores the row number in train_ind

train=mydata[train_index,] #creates the training dataset with row numbers stored in train_ind
test=mydata[-train_index,]  # creates the test dataset excluding the row numbers mentioned in train_ind

#Logistic Regression Model Estimation
mylogit_train<-glm(default2~student2+balance+income,data=train,family=binomial(link="logit"))

#coefficients
summary(mylogit_train) 

#Predict using Test data
preddata_test<-with(test,data.frame(ID,default,student,balance,income,default2,student2))
probdefault_test<-predict(mylogit_train,newdata=preddata_test,type="response")
preddefault_test<-ifelse(probdefault_test > 0.5, 1,0) #at what level should we say prob(default)=1

missclass_test<-preddefault_test!=test$default2
misclasserror_test<-round(mean(preddefault_test!=test$default2),4)
print(paste('Accuracy',1-misclasserror_test))

```
What about deviance?

The difference between the null deviance (of the null model, i.e., model with only intercept) and the residual deviance shows how the model performs against the null model. Analyzing the table we can see the drop in deviance when adding each variable one at a time. The steeper this drop, the better. Think of this as a variable selection approach.

```{r}
anova(mylogit, test="Chisq")
```




K-Fold Cross Validation
```{r}
set.seed(20)
cv.error.10=rep(0 ,10)
for (i in 1:10){
  glm.fit=glm(default2~student2+balance+income,data=mydata,family=binomial(link="logit")) 
  cv.error.10[i]=cv.glm(mydata,glm.fit,K=10)$delta[1]
}
cv.error.10
```



Does upsampling improve things? What is upsampling?
It is the practice of improving estimates and predictions by resampling the data 


```{r}
mydata$default=as.factor(mydata$default)
mydata2<-upSample(mydata, mydata$default)

mylogit2<-glm(default2~student2+balance+income,data=mydata2,family=binomial(link="logit"))

#first get predicted values
preddata2<-with(mydata2,data.frame(ID,default,student,balance,income,default2,student2))
probdefault2<-predict(mylogit2,newdata=preddata2,type="response")
preddefault2<-ifelse(probdefault2 > 0.5, 1,0) #at what level should we say prob(default)=1

confMat2_2<-confusionMatrix(data = as.factor(preddefault2),reference = as.factor(mydata2$default2),positive = "1")
confMat2_2 ###Note, because of how this matrix is strutur

```


Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Ctrl+Alt+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Ctrl+Shift+K* to preview the HTML file).

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.
